[CI]Fix test nightly workflow. (#3603)
Reapply "[MoE] [Refactor] Remove manual memory cleanup (#3365)" (#3483) (#3365)
fix : support chunked_prefill with deepseek_mtp (#2711)
[Misc] clean up useless function (#3348)
[Feat] Dynamic Batch Feature (#3490)
[Feat] add native kvcache offload (#3433)
[CI] Multi-Node CI scalable (#3611)
clean up uesless ut test (#3622)
unify logic between aclgraph and torchair (#3560)
[Fix] Fixes attribute error in MLA implementation (#3618)
[BugFix][main] Fix quantization related mtp bug with patch (#3620)
[Feat] Prefetching Attention QKV Linear Weight With `AddRmsNormQuant` Custom Op (#3517)
[Doc] Upgrade docker run command (#3645)
[main][refactor] refactor SequenceRowParallelOp forward (#3616)
[Doc] Update the modelslim website from gitee to gitcode. (#3615)
[BugFix] fix deepseek torchair precision (#3624)
perf : optimize memory for deepseek mtp (#2713)
[Misc] Add a model loader that utilizes HCCL for weight loading (#2888)
[TEST]Add initial multi modal cases for nightly test and deepseek-r1 tests (#3631)
[Structured Output] Replace `apply_grammar_bitmask()` method with that in vllm to avoid maintenance (#2524)
[Bugfix] fix delay free prefill req & D node support prefix cache (#3607)
[Doc] Update the Pangu Pro MoE tutorials. (#3651)
[Test] add a new Qwen3-32b-int8 test case with feature_stack3 (#3676)
[main][bugfix] Add 'layer_type' param to get_pergroup_param() for compatibility (#3682)
[BugFix]fix deepseek torchair recompile (#3678)
support cp&dcp (#3260)
[MoE][Multistream] Avoid performing communication in extra stream. (#3582)
[Benchmark] Upgrade benchmark args for new vllm version (#3218)
[UT] Fix test_sample_recovered_tokens_pytorch_autoregressive (#3434)
remove useless code (#3685)
[TEST]Add initial prefix cache case for nightly test (#3709)
[1/N][Refactor] Refactor code to adapt with vllm main (#3612)
[BugFix] Check all expert maps when using muilty instance. (#3576)
[TEST]Add initial multi modal cases of Qwen2.5-VL-32B-Instruct for nightly test (#3707)
[Bugfix] The server fails to locate the request, leading to the server hanging. (#3703)
[Main][Perf] Add fused matmul/reduce-scatter kernel for performance optimization. (#3693)
[Refactor] Refactor Ascend attention implementation forward (#3714)
[Feat] Add mrope fusion op (#3708)
[BugFix][P/D] Modify the recalculation logic to prevent waiting requests from filling up the D node KVCache (#3641)
[CI][Doc] Optimize multi-node CI (#3565)
[UT][fix] Add missing get_ascend_config mock to NPUWorker initialization tests (#3729)
[Misc] Limit ray version (#3660)
[CI] Skip ops test for e2e (#3665)
Update version doc (#3599)
[BugFix][Core] Fix a bug running multi-modal with ascend_scheduler (#3675)
[Bugfix] Fix zero attention output in qwen3-next (#3572)
[Refactor] optimize _prepare_inputs method in eagle_proposer (#3296)
[BugFix] Comment out newly added vlm e2e. (#3736)
[Test] Add e2e test and accuracy test for Qwen3-Next-80B-A3B-Instruct (#3450)
[Doc] Update supported models (#3481)
[Refactor] [MoE] Rename moe-related classes & files (#3646)
[Test] add test for prefix cache feature of deepseek (#3733)
[bugfixfix] correct _register function place for mooncacke (#3747)
Upgrade to new vllm commit (#3719)
[main] remove dbo code (#3712)
add qwq testcase (#3757)
[BugFix] Fix Qwen3-next break (#3428)
[Installation] limit opencv-python-headless version to resolve numpy version conflict (#3713)
[feat]dcp pcp support aclgraph (#3731)
[CI] Add custom op to nightly (#3765)
[CI] Enable 2 jobs for nightly test (#3781)
Bump actions/download-artifact from 5 to 6 (#3787)
Bump actions/upload-artifact from 4 to 5 (#3786)
[Doc][Example][Bugfix] Elements in local_device_ids should be casted … (#3782)
[bugfix][main]fix proxy decode bug (#3750)
[MM][Doc] Update online serving tutorials for `Qwen2-Audio` (#3606)
support prefill cache mode use fia op (#3696)
[Doc] Update FAQ (#3792)
【Bugfix】bugfix for weight load of  kimi-k2 (#3798)
[TEST]Add 2P1D multi node cases for nightly test (#3764)
[CI] Add multi-node test case for a2 (#3805)
Upgrade to 0.11.1 newest vllm commit (#3762)
[CI] Fix nightly CI (#3821)
[Main][Bugfix]Avoid using the fusion operator in the MOE model (#3834)
[TEST]Add aisbench log and A2 cases (#3841)
[long_seq_optim] BSND to TND and FA_UPDATE replacement (#3778)
[P/D] force with_prefill true after allreduce in kv producer (#3768)
fix qwen3next full graph break. (#3812)
[Doc] Update doc (#3836)
[HybridKV][Bugfix] Fix Hybrid kvcache sharing bug in same attention type (#3760)
[Bugfix] [MoE] fix error in deepseek when using allgather (#3824)
[Perf] Delete redundant operations in model_runner and forward_context (#3677)
[CI]pin vllm commit id (#3861)
[CI] Optimize nightly CI (#3858)
[BugFix] deepseek torchair adapt for torch_npu version (#3862)
[CI]Fix eplb nightly tests. (#3863)
fix mooncake layerwise connector (#3849)
bugfix for mtp fullgraph (#3845)
[BugFix] Fix mlapo accuracy problem related with weight processing. (#3850)
[CI]Fix oom of deepseek-eplb nigtly test. (#3884)
Add FAQ for docker pull error on Kylin OS (#3870)
[UT] fix skip ut test for test_utils (#3803)
[Doc] Remove modeling doc (#3789)
[Build] Force torch version (#3791)
[FEAT] Refactor spec decode to support efficient padded speculation (#3528)
[Model][3/N] Refactor sfa into mla and remove deepseek_v3_2.py (#3769)
[feature] Prompt Embeddings Support for v1 Engine (#3026)
[TEST]Add MALPO for aclgraph in nightly test (#3894)
[BugFix]Fix group list type of mc2. (#3864)
[bugfix] layerwise D first plan (#3866)
[CI] Optimize nightly CI (#3898)
[bugfix]cancel tokenize  for layerwise_proxy (#3914)
add new e2e tests case for aclgraph memory (#3879)
mfix bug when max_seqs=14 in mtp=2 scenario and raise error when cudagraph_capture_sizes can't be an integer multiple of uniform_decode_query_lentp (#3910)
[Bugfix] Fix MTP support for lmhead_tensor_parallel_size (#3915)
[Test] Add new test model for aclgraph single_request (#3888)
[main][bugfix] fix valueError in static_forward_context when prefix is empty (#3924)
[E2E][MM] Add e2e tests for InternVL model (#3796)
[feature] support pcp + mtp (with pd disaggregate) (#3822)
[Doc] Update doc for release notese (#3853)
Update torch-npu version to 2.7.1 (#3896)
[CI][Nightly] Correct the commit hash available for mooncake (#3943)
[Perf] Move attention update stream out of loop to optimize performance (#3848)
[Feat][UT] Support Deepseekv32 FULL_DECODE_ONLY mode and add unit test of sfa_v1 (#3763)
correct bug to fix the value of max_num_tokens (#3933)
[CI][Nightly] Fix mooncake build (#3958)
[Test] Add new e2e test use deepseek-v2-lite in ge graph mode (#3937)
revert TND modify when dcp pcp (#3948)
Quality enhancement: Immediately interrupt execution when memory OOM (#3932)
[Test] Add accuracy test for qwen3-8b-w8a8 (#3799)
[BugFix] Fix deepseek v3.2 mtp bug. (#3900)
[Test]Add accuracy test for multiple models (#3823)
[PD Disaggregation]Set adxl engine as default backend and update README (#3761)
[TEST]Add full graph for multimodal nightly tests (#3968)
[Perf] move quant before allgather in Allgather EP (#3420)
[ModelRunner][Refactor] Refactor kv cache tensor initialization logic (#3106)
[Test] Add accuracy test for qwen3-30b-a3b-w8a8 (#3807)
[Doc] Refactor the DeepSeek-V3.2-Exp tutorial. (#3871)
support qwen3-next full_decode_only mode. (#3949)
[docs] add aclgraph developer guide (#3683)
[Doc] Update version policy (#3999)
[Doc] add mtp doc (#3770)
[docs] Add kv pool developer guide (#3752)
[Doc]Add developer guide of eplb. (#3759)
[main][doc][kv_pool]Add adxl timeout parameter in kv pool user guide (#4012)
[Test] Refactor accuracy test to nightly test (#3814)
[P/D]Make kv-transfer env variable take effect & Fix load-balance proxy (#3981)
[Feat](Mooncake) Supports multiple input suffixes for global_segment_size (#3690)
[feat]decode convert bsnd to tnd and fix bug when pcp and dcp (#3980)
[TEST]Update nightly acc test standard (#4032)
[CI] Quick fix mooncake for nightly-ci (#4028)
[Bugfix] Add constraints for sequence parallelism (#4014)
[BugFix][main] Adapted to torch_npu.npu_fused_infer_attention_score (#4025)
[main][bugfix] Fix a rare bug triggered by _npu_paged_attention in FULL_DECODE_ONLY mode (#3986)
[long_seq] fix A2 accuracy problem (#4030)
[Feat] update op for mla (#4000)
[UT] Add new ut case for aclgraph in auto enable (#4031)
[Doc] Add model feature matrix table. (#4040)
[Feat] Adapted mtp function to Qwen3-next (#3918)
[BugFix]Fix group list type of mc2. (#4047)
[CI]Fix eplb ci. (#4052)
[Bugfix] fix sleepmode level2 e2e test (#4019)
[P/D][BugFix]Fix proxy format processing errors & Layerwise connector performance optimization (#4043)
[BugFix] Improve the performance of prefixcache features (#4022)
[Bugfix]fix pcp dcp attn aclgraph (#4066)
[Info][main] Corrected the errors in the information (#4055)
[TEST]Add qwen3-235b-w8a8 and qwen3-30b-w8a8 nightly test (#3973)
[Doc] Remove extra MLAPO installation step for DeepSeek-V3.2. (#4024)
[docs] [P/D] add feature guide for disaggregated-prefill (#3950)
[Feat] flashcomm_v2 optim solution (#3232)
[Feature][Build] Upgrade the minimum version to 3.10 (#3926)
[Fix]  fix Qwen2-Audio-7B-Instruct accuracy test (#4017)
[Typo] LLama has been changed to Llama (#4089)
[Core] Restore scheduling logic under default configuration (#3967)
[Doc] add qwen3 w4a4 tutorial (#4076)
[BugFix] Fixes Qwen3-Next enable nz accuracy problem (#4058)
[Doc] Add release note for v0.11.0rc1 (#3931)
[main][Bugfix] Fix ngram precision issue and open e2e ngram test (#4090)
[feature] chunkprefill support pcp&dcp (#3801)
[Doc] Recover installation doc to use pip install (#4109)
[Test] Add nightly test for DeepSeek-V3.2-Exp (#3908)
[Fix] Refactor and fix dist test to e2e full test (#3808)
[Fixbug] Fix ut test (#4116)
Remove VLLM_USE_V1 (#4086)
[CI] Integrate mooncake to vllm-ascend base image (#4062)
[TEST]Update nightly cases and add mtpx (#4111)
oproj TP support acl graph (#4073)
[Test][Accuracy] Add accuracy evaluation config for InternVL3_5-8B (#3964)
[Misc][Doc] Add service profiling feature with user guide (#3756)
[Perf] Remove D2H operations to imporve performance (#4063)
[Doc] Fix DeepSeek-3.2-Exp doc, remove v0.11.0rc0 outdated infos. (#4095)
fix fullgraph in ds. (#4016)
[feature] support pcp + mtp (in pd co-locate scenario) (#4098)
[main][bugfix] Change seq_lens in dummy attn_metadata to max_query_len (#4097)
[CI] Fix nightly-ci (#4159)
Upgrade to 0.11.1 newest vllm commit (#3982)
[Perf] fix async copy for async scheduling (#4113)
[Perf] [MoE] optimize all2allv (#3738)
[Bugfix] fix mtp profile run error where main model and mtp model use different quantization (#4102)
[BugFix] adapted e2e tests for Qwen3-next-mtp (#4160)
[BugFix] Fix kv_no_split not contiguous (#3594)
[Info][main] Correct the mistake in information documents (#4157)
[Test]Add ut test qwen3_moe and sfa (#4121)
[CI] Remove unsupported python 3.9 format check (#4172)
[CI] Add daily images build for nightly ci (#3989)
[long_seq_Feat] support chunk prefill (#4158)
[CI] Add multi-nodes EPLB configs of DeepSeek-R1-W8A8 & Qwen3-235B-W8A8 (#4144)
[Bugfix] fix cannot import name get_mp_context (#4174)
[Feat] Adds a utility for printing from within ACL graphs (#4162)
[Platform] Add import_kernels interface (#3694)
[Misc] Add benchmark results into `.gitignore` (#4200)
[Test] Add deepseek v3.2 exp nightly test (#4191)
[CI] Fix no space left in build wheel CI. (#4215)
support FULL graph mode for GQA (#3970)
[TEST]Update prefixcache perf threshold for qwen3-32b-int8 (#4220)
make vllm-ascend work well in developer mode (#4179)
[Bugfix]Fix moe error when sp chunked the hidden_states (#4212)
[main][misc]change default capture size for Qwen3-MoE when using full dp (#4199)
[feature] Mooncake_connector support pcp/dcp (#4183)
[P/D] pd proxy support ipv6 (#4161)
[bugfix] fix proxy hen host ip using domain name (#4243)
[Fix] Sorts aclgraph batch sizes in ascending order (#4230)
[refactor]support gatingtopk operator generalization (#2958)
[bugfix] pcp + mtp acl graph bugfix (#4221)
[CI] Fix kubernetes failed to resolve ip by dns name (#4240)
[Bugfix] fix hang in async scheduling (#4233)
remove get_metadata_cls (#4087)
[doc]fix readme for kv pool user guide (#4271)
[Docs] Improve the AISBench multi-modal testing docs (#4255)
[misc] clean up get_metadata_cls (#4276)
[long seq feat]GQA support long-prefill-token-threshold and fixbug (#4209)
[Bugfix] fix nightly multi-node EPLB tests' "DYNAMIC_EPLB=true" environment not working (#4223)
avoid mrope fusion op when running qwen2.5-vl on a+x machine (#4270)
[Test] Add tests for the multi-node DeepSeek-V2-Lite network in GE Graph (#4039)
[CI] Add mla ut (#4280)
[Feat] Support MTP to running in full graph mode (#3892)
[Test] quick fix mla ut (#4318)
[Test] Add ACL graph capture/replay DP test (#4259)
[Feat][BugFix]Support the Qwen3-Next-80B-A3B-Instruct quantization model&Fix the NZ issue (#4245)
eplb redundant expert bugfix (#4291)
[Readme] EPLB Support Scenarios (#4314)
[MM][Bugfix] Add error log for VL models when enabling FLASHCOMM (#4272)
[Feat][Doc] Add a load_balance_dp_proxy in examples and external dp doc. (#4265)
[Test] Add ut test for torchair (#4287)
[bugfix] bugfix for PD disaggregate (#4319)
[EPLB] Eplb Verify Fix (#4333)
[Doc] add release note for v0.11.0rc2 (#4348)
[BugFix] Fix some issues caused by the ascending order of cudagraph_capture_sizes (#4338)
[Bugfix][KV Pool]fix get_ip import in mooncake_store (#4355)
[Doc]Add single node PD disaggregation instructions (#4337)
[CI] Fix nightly CI for A2 series (#3825)
[Doc] Upgrade multi-node doc (#4365)
Change the first letter to uppercase (#4375)
[Fix] Remove unnecessary NPU synchronization in MTP proposer (#4325)
[TEST]Update deepseek mtpx acc cases standard (#4321)
Drop 0.11.0 support (#4377)
[Fix] fix aclgraph e2e test. (#4131)
[Refactor] remove moe type of multicast. (#4224)
[Bugfix][MoE] enable force_load_balance in aclgraph (#4366)
[feature] vllm-ascend support msprobe (eager mode dump) (#4241)
Bump actions/checkout from 4 to 6 (#4380)
[Bugfix]Fix the hang issue of multimodal model when running with DP>1 (#4392)
Document error correction (#4422)
[Bugfix] fix patch typo (#4351)
[bugfix]Return the Transformer version from 4.57.2 to 4.57.1 (#4423)
[Bugfix] use module-level import for patched function in Qwen3Next (#4354)
[MM][Bugfix] Minor fix for VL model verification (#4384)
[misc] Remove useless patch_logits (#4252)
[TEST] Delete Comment (#4427)
mkdir triton package and move triton files (#4420)
upgrade to vllm 0.11.2 (#4400)
[CI] clean up ci (#4452)
[refact] unified soc_version code (#4359)
Change comment location (#4432)
[UT] Fix ut test (#4472)
chip type judgement code optimization (#4485)
[CI][Nightly] Support local debugging for multi-node CI test cases (#4489)
[BugFix] Adapted Qwen3-Next eager mode to v0.11.2 (#4477)
[bugfix] fix ray start failed: local_world_size cannot little than visible device count error (#4457)
[feature] Add Custom Op grouped_matmul_swiglu_quant (#4431)
[TEST] Add eagle proposer ut (#4447)
[main]Upgrade cann to 8.3rc2 (#4350)
[Quantization] Support compressed tensors w8a8 static and w8a8 dynamic weight (#4036)
[MM][Model][Perf] Remove Qwen2.5-VL modeling files and add patch for VisionAttention (#4349)
[P/D] Add readme for PD separation (#4182)
[Doc]Delete equals sign (#4537)
[Kernel] add custom op GmmSwigluQuantWeightNzTensorList (#3804)
[Feature][main]reconstruction kvpool connector to ascend connector (#4438)
【OPS】qwen3-next  support triton chunk_gated_delta_rule ops (#4070)
update triton package url (#4552)
[Bugfix] Fix model run _npu_flash_attention hang issue (#4410)
[Doc] Add single NPU tutorial for Qwen2.5-Omni-7B (#4446)
Update triton package name (#4563)
[bugfix] dep ineffective (#4417)
[P/D] [bugfix] add get_kv_connector_handshake_metadata func for 0.11.2 (#4567)
drop ascend scheduler (#4498)
improve soc version (#4522)
[MM][Model] Remove Qwen2-VL modeling files (#4534)
Move mla to ops module (#4575)
[Bugfix] fix dp parallel + tp > 1 offline inference port conflict (#4539)
remove qwen3-next model file (#4573)
[feature]Pooling Features and PCP Adaptation (#4143)
Revert "drop ascend scheduler" (#4580)
[CI] Skip test_ngram_correctness as the oom issue block CI (#4578)
[bugfix] Repair the problem of moe model accuracy caused by version upgrade. (#4562)
[Bugfix] Fix kvpool precision synchronization (#4574)
[feature] Support W8A8 PD-Mix Quantization (#4235)
[EPLB][Ops] Integerate grouped_matmul_swiglu_quant_weight_nz_tensor_list operator into dynamic EPLB (#4216)
[OPS] add bmm_transpose ops (#3990)
[BugFix] Fix Qwen2.5_Omni vision customized op attr err (#4568)
[Bugfix] Resolve MTP > 1 issue when lm head tp > 1 (#4254)
Bump actions/setup-python from 6.0.0 to 6.1.0 (#4591)
[Bugfix] Fix bug with establishing the flashcomm2 and pp communication domains. (#4458)
[Kernel] add triton kernels for sampling (#4550)
add _cann_ops_custom gitignore (#4605)
[Feature] Integrate Suffix Spec Decoding (#4045)
upgrade torch npu version (#4433)
[Bugfix] PCP adaptation for VLLM v0.11.2 modifications (#4604)
[Bug_fix] fix torchair o_proj forward parameter (#4166)
[CI] drop ascend scheduler test (#4582)
[Feat] shared expert dp for deepseek_mtp (#3811)
fix qwenvl pd smoke test error (#4597)
[Test] Add accuracy nightly test for new models (#4262)
[Doc] Fix DeepSeek-V3.2-Exp doc, add docker command. (#4479)
[Test] Add GLM-4.5 nightly test (#4225)
[Bugfix] Remove ModelSlim-"M4 Quantization". (#4589)
[main][bugfix] bugfix for qwen3 moe quantization (#4599)
[MM][Model] Remove Qwen3-VL modeling files (#4577)
[CI]enable chunked prefill by default (#4569)
[Refactor] Remove redundant attention operator branches. (#4531)
[Bugfix] Fix Qwen2.5-Omni-7B accuarcy test (#4556)
[Bugfix]Fix eplb enable when using mtp float weights. (#4571)
Bump actions/checkout from 4.3.1 to 6.0.0 (#4592)
Revert "[Bugfix] Fix Qwen2.5-Omni-7B accuarcy test (#4556)" (#4556)
[CI] Drop ascend scheduler from test (#4613)
add hyperlink (#4588)
[Doc]clean up ascend scheduler config from doc (#4612)
[Doc] Add tutorial for Qwen3-Coder-30B-A3B (#4391)
[Ops][Triton] Add a triton kernel supporting partial rope. (#4413)
clean up model module (#4611)
[Doc] Refactor the DeepSeek-V3.1 tutorial. (#4399)
[performance] Enhance performance after enabling min_p (#4529)
【doc fix】doc fix: deepseekv3.1 (#4645)
[Bugfix] fix custom op GmmSwigluQuantWeightNzTensorList (#4593)
upgrade vLLM to main (#4608)
[kernel] add AscendC op: lightning_indexer and sparse_flash_attention (#4625)
[Doc] add release note for v0.11.0rc3 (#4646)
fix typo (#4657)
[Model] Add qwen3Next support in Main (#4596)
[Feat] MTP support DeepSeekV3.2 (#4465)
[Fix] Fix FIA `query` and `query_start_loc` shape mismatch error (#4518)
[CI] Fix ut ci: no space on the device (#4662)
[Misc] Add cann custom ops to `.gitignore` (#4670)
fix custom ops env set error (#4675)
[Core] Encoder separation for Encode-Prefill-Decode Disaggregation (#4176)
upgrade vLLM to 0.12.0 tag (#4647)
Remove cancel for main to main check (#4685)
Adopt inductor fusion and define quantization fusion pass (#4168)
Remove ascend schuduler ut (#4684)
【fix】ops gatingtopk fix nightly ci error (#4340)
[MM][Patch] Remove patch for cos/sin cache (#4672)
[Nightly] Optimize nightly CI (#4509)
[Misc] Upgrade vllm vllm commit to 2025_12_04 (#4690)
add `dispatch_gmm_combine` kernel (#3532)
[Bugfix] Quick hot fix for nightly CI (#4727)
[Doc] Update vLLM version in doc (#4691)
Drop ascend scheduler (#4623)
[long_seq] remove long_seq env (#4660)
Update comment doc (#4731)
[BugFix][Triton] Fix ub overflow bug of sample_recover_tokens_kernel (#4673)
[Bugifx] fix quant_apply_mlp w1_scale type error & fix getting num_local_expert (#4632)
[P/D][main] Clean connector history information (#4650)
[CI] Fix unit test fault `no space left` (#4728)
【main】[Doc]add 2P1D instruction for single node (#4716)
[Refactor] 1/N Refactor attention_v1 & extract attention_cp (#4628)
[Bugfix]fix bmm_transpose ops for cann version (#4653)
rm vanilla attn (#4558)
mlapo add qdown output (#4707)
[Bugfix] fix mtp and eagle aclgraph bug (#4710)
support async mtp (#4511)
[BugFix] Fix eagle3 accuracy problem when enforce_eager=True (#4521)
[Kernel] add custom op DispatchGmmCombineDecode (#4139)
[Feat]enable sfa cp for dsv3.2 (#4702)
Support DeepSeekV3.2 with MLAPO operator (#4753)
[P/D] check kv extra config and del hccl backend (#4547)
[BugFix] Refactor ACL graph size adjustment for speculative decoding (#4640)
[Feat] Add Euler xlite graph wrapper support (#4526)
fix synchronize error of exceeds_max_model_len d2h copy (#4708)
[CI] Fix ngram & suffix test oom (#4755)
Deepseek Mtp model uses the lm_head and embedding from the main model (#2790)
remove useless patch (#4699)
[Op] DeepSeekV3.2 support bmm_transpose operator (#4631)
[EPLB] Add log Info for moe_load Imbalance Ratio (#4482)
[Fix] skip xlite e2e test (#4786)
[Bugfix] Fix Dcp dimension mismatch when enable Mlapo (#4687)
[Kernel] add custom moe ops for prefill (#4194)
Bump actions/checkout from 6.0.0 to 6.0.1 (#4772)
[Doc] Add Qwen3-235B tutorial (#4358)
[DP] Fix dp padding logic in dummyrun (#4705)
[MOE]move weight transpose to wakeup for RL secnarios (#4626)
Fix incorrect MLAPO weight release in PD mixex scenarios. (#4774)
Revert "[Kernel] add custom moe ops for prefill" (#4806)
[Bugfix] Add the check for a null VllmConfig (#4749)
[CI] Skip `test_suffix_correctness` (#4820)
[Docs]fix the configuration conflicts in documentation (#4823)
[CI] Optimize CI time (#4821)
[KVPOOl]Support pp (#4761)
[Feat] Multi-stream for eplb heat collection and aggregation (#4214)
[kernel] Adapt DispatchGmmCombineDecode operator to parameters of small operators (#4790)
[CI] Increase HCCL_BUFFSIZE for A3 (#4838)
[Bugfix]fix bmm_transpose ops in dsv32 (#4791)
[UT]add pcp aclgraph ut (#4804)
[Usability]local_buffer_size support for units: GB, MB, KB, B (#4829)
[Refactor] 2/N Unify all mask generation methods and cache mask (#4779)
[CI] Setup github proxy for self_hosted runners (#4841)
[Fix] Add extra warmup run count for MC2 on specific SoC version (#4843)
[Bugfix] Disable the dispatch_ffn_combine kernel in MTP path (#4751)
[P/D][main]Offline the llmdatadist connector related parts of the code and files. (#4780)
Add gsm8k accuracy test for multi-note Qwen3-235B-A22B (#4802)
[bugfix] fix quant method validation bug (#4831)
[Kernel] add custom op MatmulAllreduceAddRmsnorm (#4606)
Drop torchair (#4814)
[Nightly] Optimize nightly online test logger info (#4798)
[Test] Temporarily skips Qwen3-30B-A3B-W8A8 data parallel test case (#4857)
add e2e test for mtp async_scheduling (#4826)
[Model] Support pooling models (#3122)
[CI]Cleanup accurary test (#4861)
[CI] Use offline mode for modelscope (#4875)
[Feat] Support native Kimi-K2-Thinking native W4A16 quantized experts weights (#4516)
mooncake connector support pipeline parallel & fix pp with flashcomm1 (#4054)
add multi_npu_qwen3_dense tutorials (#4543)
[CI] fix lint (#4888)
[Kernel] Add moe normal ops (#4810)
[Bugfix] Fix out-of-bounds access to token_id due to uninitialized logprobs (#4248)
[FEAT] Support DeepSeek-V3.2 with `FULL_DECODE_ONLY` mode (#4706)
Fixed the performance degradation issue in post-processing in speculative decoding scenarios. (#4849)
[Bugfix]  Support for mlapo in deepseekv3.1 w4a8 (#4828)
[Feature] Support npuhraph_ex backend (#4700)
[perf][dsv3.2][async_scheduling] improve dsv3.2 performance by eliminating HD synchronization (#4805)
[BugFix][main] Adapted Qwen3-Next-MTP to chunked prefill (#4770)
Update patch doc (#4869)
Remove COMPILE_CUSTOM_KERNELS env (#4864)
Remove VLLM_ASCEND_ENABLE_TOPK_TOPP_OPTIMIZATION (#4860)
Remove useless env (#4858)
add DeepSeek-R1 tutorial. (#4666)
[Doc]Add tutorial document for qwen-VL-Dense (#3516)
[Doc] Add local running multi-node nightly test case guide (#4884)
[E2E] Remove unused PD-disaggreate scripts in E2E test. (#4837)
[E2E] Refactor the e2e testcases. (#4789)
[E2E] Optimize nightly testcase. (#4886)
[feat] mlapo add bf16 no_quant support (#4852)
cleanup useless torchair logic (#4856)
[Feat] Flashcomm2 use o_shared linear (#4188)
[OPS] support triton causal_conv1d_fn ops (#4119)
【Bugfix】bugfix_for_bmm_transpose (#4899)
[Bugfix] Fix the bug in sfa-cp under multi-DP scenarios. (#4850)
[feat] apply flashcomm1 on bailing (#4868)
[Bugfix] support mtp kv transfer and pp partition by hand in kv transfer (#4892)
[doc]  Add Qwen2.5 tutorials (#4636)
[Fix] Delete redundant variable (#4903)
[Fusion] normalize fusion naming and enable e2e test (#4693)
[Bugfix] Prevent engine hang during KVCacheSendingThread startup (#4754)
[CI] speed up ut (#4901)
Remove mindie_turbo (#4896)
[Doc] Update structured output doc with upstream link (#4015)
Refactor CI workflow (#4912)
[CI] Cancel whl build when submitting a new commit (#4925)
[CI]cleanup e2e test (#4800)
[Doc] Update tutorial index (#4920)
[bugfix][refactor] fix recompute_scheduler break with vllm 0.12.0 & support async scheduling & refactor recompute_scheduler.py (#4895)
[Performance] Pre-issued exponential distribution operator. (#4908)
[CI] refect e2e test (#4799)
[MoE][TorchAir] Remove FusedMoEState (#4927)
[main][Bugfix] Remove the ZMQ communication setup on the D node (#4926)
[Feat] Add custom Embedding tensor model parallel (#2616)
[Bugfix] bugfix for moe_mlp (#4822)
BugFix: Resolve PolicyFlashlb warm up function attribute error (#4741)
[CI] fix light test (#4954)
【doc】Add model feature matrix (#4950)
[Doc] Upgrade outdated doc (#4957)
update qwen2.5vl readme (#4938)
vllm-ascend support Ascend950 with Qwen dense model. (#4228)
[usability]Modify the default value of the protocol to ascend (#4959)
[Nightly] Remove gen_ranktable logic (#4941)
[Feature] model_runner refactor (#4764)
[doc][main] Correct mistakes in doc (#4945)
[CI] Add mtp_proposer ut (#4397)
[Bugfix] Pass vllm_config to kv_connector_no_forward in NPUModelRunner (#4970)
[Bugfix] fix eagle proposer (#4971)
[bugfix] asyncscheduler bug fix (#4968)
[doc][main] Correct more doc mistakes (#4958)
Revert "[Bugfix] support mtp kv transfer and pp partition by hand in kv transfer (#4892)" (#4892)
[perf] replace all_reduce for kv_consumer and support different num_tokens among all ranks (#4983)
[CI] Pull latest vllm-ascend src before tests (#4988)
add release note for 0.12.0 (#4995)
[Fix] Fixes issues in MTP with async scheduling and ACL graph (#4963)
[Perf]enable prefill flashcommon3 (#4065)
[CI] CI refactor (#4928)
add ut for model runner (#4991)
[Misc] Update pooling example (#5002)
[CI][Bugfix] Fix scheduleroutput has no attr get error in prompt logprobs (#4998)
Add Qwen3-Next tutorials (#4607)
[Refactor]3/N Refactor mla_v1.py & extract mla_cp (#4933)
[main][BugFix] Fixed an accuracy bug of Qwen3-next-MTP when batched inferring (#4932)
Bump actions/upload-artifact from 5 to 6 (#5014)
[bugfix] Fix dummy-run and multi-node issues in MoE routing and MTP (#4947)
[Doc ] Supplement kvpool user guide (#5013)
[Test]update accuracy test of models (#4911)
[Bugfix] Fix the bug in initializing the shared_weight communication domain in sfa-cp, and fix the mtp weight load in pp>1 situation (#4913)
[Bugfix] Add support for PP intermediate value types in graph mode (#4902)
[Bugfix] qwen3-vl-235b-w8a8 load weight ERROR when start service (#4292)
update release note for suffix decoding (#5009)
[Graph][Fusion] Add AddRMSNorm(with bias) and Quant Fusion Pattern (#5011)
[UT]add pcp dcp ut (#4949)
[Bugfix] fix the incorrect use of python's sum on tensors. (#4655)
[Misc] Upgrade vllm hash to 12_14 (#5000)
[CI] Delete deepseek3.2-exp nightly test (#5028)
[E2E] Collect test run time. (#5018)
[doc]Modify quantization tutorials (#5026)
[KVPool]Fix PP get bug (#5007)
[Attention] Temporarily add back pa for small batch sizes. (#4765)
[Cleanup] Remove unused attn_metadata parameter from Proposer classes (#4862)
[bugfix] [main] Fix KV cache query inconsistency across different TP ranks in the KV Pool (#5030)
[Bugfix] Fix precision issues in moe_mlp (vllm-ascend main) (#5025)
[Bugfix] Fix the attn_metadata is None (#5038)
[Misc] Upgrade vllm commit hash to 1215 (#5029)
[Fix]Revert temporary skip on mtp1/mtp2 correctness tests (aclgraph fix) (#5039)
[Core][Worker] Add UCMConnector for KV Cache Offloading (#4411)
[Bugfix] dynamic eplb does't use fused_alltoall (#4919)
Bump actions/checkout from 4 to 6 (#5015)
[Feat] Refactor rejection sampler (#4975)
[bugfix] Fix mooncake kvpool accuracy issue (#4976)
[Refactor] Remove the process patches of Qwen2.5-VL and Qwen2.5-Omni (#5035)
[Doc] Upgrade some outdated doc (#5062)
[ModelRunner] apply_grammer uses vllm function (#4974)
[Bugfix] fix fastapi version (#5047)
[BugFix]Fix FIA input err in DSv3.1 (#5059)
[Doc] Add user guide of speculative decoding (#5074)
Add release note for v0.11.0 (#4918)
[bugfix] matmul_allreduce_add_rmsnorm aclnn interface (#5082)
【Feature】refactor npu_modelrunner for profile_run (#4993)
Add a Mooncake installation tutorial for kv pool and update Mooncake installation tutorial (#5069)
[Bugfix] EPLB nightly deepseek (#5095)
[Nightly] Upgrade single node test to latest main (#5101)
[Nightly][BugFix] Install triton for nightly e2e op test. (#5096)
[Feat] Support async_scheduler and disable_padded_drafter_batch in eagle (#4893)
[bugfix] fix mtp accept rate (#5093)
Upgrade vllm commit hash to 1216 (#5053)
[Fusion] [Graph] Add qknorm rope fusion operator (#4711)
[UT]add the UT of pcp and dcp in the attention_cp file (#5054)
[Bugfix] Fix DeepSeek FIA error in async_scheduling with mtp (#5046)
[feat]pd disaggregated support cross-machine (#5008)
[CI] Fix UT (#5106)
[main] rename device type (#5099)
[main][doc] Instructions for using permissions added to docker (#5092)
[Pangu][MoE] Remove PanguProMoEV1 related code (#5088)
[model] Support PanguUltraMoE (#4615)
[UT] add pcp&dcp UT for mla_cp (#4953)
[BugFix] Fix mooncake bug in PCP scenario (#5055)
[Bugfix][MoE] Remove All2All in w4a8_dynamic (#4977)
Fix a data conversion bug introduced by commit 3b7eb51 in main#4655 (#5115)
[Refactor] 4/N Distinguish the branches based on the applicable scenarios of PA and FIA Ops. (#5081)
[Bugfix]delele profile_run in model_runner (#5122)
[Fix] Synchronize the host query_start_loc with device values to prevent shape mismatches (#5134)
fix profile run for vl model (#5136)
enable npugraph_ex (#5120)
[Doc] add qwen3 reranker (#5086)
[UT] Add model_runner pcp related UTs (#4951)
qwen3_next add triton ops : fused_qkvzba_split_reshape (#4788)
[test] add w4a8 accuracy case (#5110)
[feat] proxy support elastic scaling (#5063)
[Fix] Fix DeepSeek V3.2 "no attr" error (#5147)
[UT]Ut for function cumsum_group_list  in moe_mlp (ref #5025) (#5036)
[UT] Add mooncake ut test (#5080)
fixed fused alltoall execute all reduce (#5109)
Qwen3-Next：Update the gpu-memory-utilization parameter to 0.7 (#5129)
[Bugfix] fix pipeline parallelism bug introduced by async-scheduling refactor work (#4973)
implement model runner v2 basic framework (#5051)
fix: use batch_matmul_transpose operator in MLA _v_up_proj for better performance (#5142)
Nominate new maintainers @zzzzwwjj @realliujiaxu @LCAIZJ (#5152)
feat: implement high-performance Triton kernels for rejection sampling (#4830)
[Feat] Support MLP_TP feature, exclude MOE layer (#4999)
[Graph][Fusion]Add new pattern for AddRmsnormQuant with SP. (#5077)
[Fix] Refines decode mode padding condition for uniform queries (#5164)
fix vl pd smoke error (#5103)
[BugFix]Fix incorrect get_current_vllm_config (#5121)
[Nightly]  Avoid max_model_len being smaller than the decoder prompt to prevent single-node-accuray-tests from failing (#5174)
[Doc] Refact benchmark doc (#5173)
[Bugfix] Fix in_profile_run in mtp_proposer dummy_run (#5165)
[Doc][P/D] Fix MooncakeConnector's name (#5172)
[BugFix] Fix top_p,top_k issue with EAGLE and add top_p,top_k in EAGLE e2e (#5131)
[bugfix] Use FUSED_MC2 MoE comm path for the op `dispatch_ffn_combine` (#5156)
[2/N][Pangu][MoE] Remove Pangu Related Code (#5130)
[Bugfix] install trition for test_custom_op (#5112)
support basic long_seq feature st (#5140)
【Doc】Deepseekv3.1/R1 doc enhancement (#4827)
[BugFix]Fix precision issue for LoRA feature (#4141)
[refactor] refactor weight trans nz and transpose (#4878)
[Image] Refactor image build (#5175)
[Doc] Add a perf tune section (#5127)
Add Qwen3-VL-235B-A22B-Instruct tutorials (#5167)
[Refactor] remove some metadata variables in attention_v1. (#5160)
[CI] Improve CI (#5078)
[Feature] Add token mask for DispatchGmmCombineDecode operator (#5171)
[pref] qwen3_next add triton ops : fused_sigmoid_gating_delta_rule_update (#4818)
[Doc]Add the user_guide doc file regarding fine-grained TP. (#5084)
restore matmul_allreduce_add_rmsnrom aclnn interface (#5119)
[CI] Fix image merge bug (#5197)
[CI] Use offline mode for nightly test (#5187)
Drop 0.12.0 support (#5146)
[CI] unblock CI on suffix spec decoding (#4813)
[e2e] add pcp e2e (#5141)
[CI] fix lint (#5216)
[lint]clean code (#5218)
[Fix] Delete pooling redundant code (#4940)
[Performance] Add async exponential while model executing (#4501)
[BugFix]Fix wrong _cos, _sin instantiation (#5154)
[Feature]Use DispatchGmmCombineDecode operator to replace MC2(Optional) (#5040)
[Perf] vectorize PCP/DCP loops in attention_cp.py (#4944)
[Perf] vectorize PCP/DCP loops in mla_v1.py (#5003)
[Misc] Cleanup useless print and logger (#5220)
[Doc] Fix DeepSeek-V3.2 tutorial. (#5190)
[bugfix][ACLGraph][MTP] deletes `cudagraph_batch_sizes` in `MtpProposer` (#5183)
[task] Add fused gdn gating triton kernel (#4304)
[CustomOp] Register AscendMMEncoderAttention CustomOp and remove related patch (#4750)
[misc][FlashComm1][ACLGraph] Incompatibility between Flashcomm1 and FULL_DECODE_ONLY. (#5200)
Bump actions/upload-artifact from 4 to 6 (#5233)
Bump actions/checkout from 4 to 6 (#5234)
[Doc] Update readme (#5226)
[1/N][Eagle3] Aligns auxiliary hidden state usage for eagle3 models (#5162)
[Triton]support swiglu_quant triton in w4a8 (#5161)
[feature] support pcp + mtp in full graph (#4572)
[Feat]Xlite Qwen3-vl Support (#5228)
[bugfix] fix w8a8dynamic fused_moe trans nz (#5199)
[Bugfix] Implement multimodal_cpu_fields in model runner (#5196)
[TEST]Update mm param --mm-processor-cache-gb (#5242)
[Bugfix] Use hf_text_config instead of hf_config to support multimodal PD-Disaggregated (#5205)
[Refactor] move the metadata from attention_v1 to util(ready for extract common_cp) & realize Ascendmetadata inherit from the parent class. (#5203)
[refactor] Remove unnecessary attributes from set_ascend_forward_context (#5204)
[Doc] Update the weight download URL. (#5238)
[Main] [Patch] support balance scheduling patch (#5212)
[Doc] Add new contributors and relative scripts. (#5070)
[CustomOp] Register AscendApplyRotaryEmb CustomOp and remove related patch (#4667)
[Doc] fix docs set rope_theta value is 10e6 in qwen3-235b model (#5258)
[ModelRunner] Add hunyuan-vl basic support (#5151)
[KV-Sharing] Support KV-Sharing feature in CLA models (#4138)
[EPLB][CI] Add dynamic EPLB CI for qwen3-moe (#5179)
[CI] Add Triton Ascend in CI (#4921)
[CI]refactor: standardize test case naming convention (#5243)
[test]Corrected the Qwen3-Omni-30B-A3B-Instruct accuracy test configuration in nightly tests. (#5195)
[main][Refactor] Remove `with_prefill` parameter from `set_ascend_forward_context` (#5094)
[Doc] Added deploying on k8s with kthena (#4674)
[CI] Mock spawn for vlm tests (#5279)
[CI] refect e2e ci test (#5246)
[Refactor][MoE] Reuse vLLM's all_reduce logic (#5189)
[Bugfix] quick fix balance scheduling patch (#5281)
update to vllm 12-19 (#5223)
fix transformer version to 4.57.3 (#5250)
[Refactor]5/N Extract common code of mla_v1.py & extract mla_cp (#5097)
[CI] Add skipped testcases. (#5254)
[E2E] Optimize e2e test. (#5091)
[bugfix] remove the EP buffer allocation introduced by fused-op dispatch_ffn_c… (#5284)
[Doc] Add pa_shape_list description to qwen dense tutorial (#5225)
Update vllm pin to 12.24 (#5307)
[CI] Skip some failed ops tests (#5309)
[perf][bugfix] improve performance of rejection sampler and eliminate HD synchronize in TopKTopPSampler (#4154)
[quantization] Add w8a16 quantization support (#4541)
Cleanup uesless env (#5270)
Revert [KV-Sharing] Support KV-Sharing feature in CLA models (#4138) (#4138)
[Kernel] add l2norm triton kernel (#4595)
Add MagicMTP(block verify) and Triton optimization (#4443)
[CI] add xlite e2e test (#5305)
[E2E Refactor] Enable skipped e2e case (#5287)
[BugFix] Fix num_pcp_pads Assignment Issues (#5273)
[bugfix] fix Error 'ValueError: Duplicate layer name' (#5280)
Remove VLLM_ASCEND_ENABLE_DENSE_OPTIMIZE (#5272)
fix e2e rejection-sampler error (#5341)
[Bugfix] fix pcp 128K break (#5266)
[Bugfix] fix xlite decode-only e2e test (#5354)
[doc] update using command (#5373)
[Bugfix] Fix Qwen P/D Disaggregation accuracy issue (#5340)
[CI] Skip failed test cases to recover CI (#5368)
[FIX] Update _causal_conv1d_update_kernel for Efficient Conv State Handling on NPU (#5322)
[BugFix][Fusion] Patch compile backend to make fusion available (#5308)
move contiguous in fused_sigmoid_gating_delta_rule_update to model_runner_v1 (#5274)
[Nightly] Initial logging for nightly multi-node testing (#5362)
Update vllm pin to 12.25 (#5342)
cleanup ascend config (#5296)
[E2E] Optimize the E2E test time. (#5294)
Revert "Add MagicMTP(block verify) and Triton optimization (#4443)" (#4443)
[doc]<PCP&DCP> add developer guide for PCP&DCP (#5372)
[Bugfix] Fix unsuitable moe_comm_type under ep=1 scenario (#5388)
[doc] Add context parallel user guide (#5358)
[Doc] update R1/V3.1 doc (#5383)
[Feature] Enhance all-reduce skipping logic for MoE models in NPUModelRunner (#5329)
[TEST]Add sending request with and without chat (#5286)
rollback causal_conv1d_fn to torch ops & update qwen3Next doc (#5391)
[bugfix] Fix MHA model runtime error in aclgraph mode (#5397)
[Feature] Remove the transpose step after attention and switch to transpose_batchmatmul (#5390)
Update vllm pin to 12.26 (#5378)
[CI] Add qwen-235b-a22b a2 multi-node test (#5393)
[Build] Add installation script of fused_infer_attention_score kernel with flash decoding (#5402)
[Test] Add acceptance test for eagle/eagle3 (#5366)
[TEST]Add vllm bench (#5306)
MLA prefill preformance optimization (#5275)
Revert "MLA prefill preformance optimization (#5275)" (#5275)
[Doc]add long sequence tutorials (#5364)
[bugfix][main]KV Pool for KV Transfer in PD Disaggregation Scenarios (#5398)
[BugFix] Fix npu-cpu offloading interface change bug. (#5290)
[Doc] modify pcp tutorials (#5411)
[bugfix] solve dp scenario Host-Device sync (#5298)
[Doc] add long_sequence feature user guide (#5343)
[Doc] delete environment variable HCCL_OP_EXPANSION_MODE in DeepSeekV3.1/R1 (#5419)
[feat] enable hierarchical mc2 ops on A2 by default (#5300)
[doc] Update Qwen3-235B doc for reproducing latest performance (#5323)
[Bugfix] fix greedy temperature detection (#5417)
Revert "[feat] enable hierarchical mc2 ops on A2 by default (#5300)" (#5300)
[Doc] Modify DeepSeek-R1/V3.1 documentation (#5426)
[DOC]Fix model weight download links (#5436)
[Doc] Update DeepSeek V3.1/R1 2P1D doc (#5387)
[Misc] fast fail for exiting if tools/install_flash_infer_attention_score_ops_a2.sh (#5422)
[Doc]modify pcp tutorial doc (#5440)
[bugfix] fix typo of _skip_all_reduce_across_dp_group (#5435)
Fix nightly (#5413)
[Bugfix] Correctly handle the output shape in multimodal attention (#5443)
[ReleaseNote] Add release note for v0.13.0rc1 (#5334)
update vllm pin to 12.27 (#5412)
[Refactor] cache cos/sin in mla & remove parameter model in builder. (#5277)
[Refactor]6/N Extract common code of class AscendMLAImpl (#5314)
[EPLB][refactor] Modification of the initialization logic for expert_map and log2phy（depend on pr5285） (#5311)
[Feature] Support to use fullgraph with eagle (#5118)
Optimize some rejectsampler functions to make npu op launch non-blocking (#4587)
[feature] fia support sliding windows (#5239)
[Feature] support eager mode in model runner v2 (#5210)
[Refactor][Triton] Move reject sample triton kernels into ops/triton (#5324)
[Refactor][EAGLE] 1/N delete __init__ in mtp_proposer (#5176)
[OP] add custom op aclnnMoeInitRoutingCustom (#5251)
[Kernel]update csrc cmakelist for open-source cann (#5458)
Update corresponding vllm commit ID to 12 29 (#5475)
[refactor] refactor model runner capture model (#5230)
moe_gating_top_k (#5271)
[CI]update triton ascend version (#5392)
[Doc] Fix issue link for 0.12.0 (#5500)
Revert "moe_gating_top_k" (#5512)
Docs: Remove deprecated --task parameter for embedding models (#5257)
[1/N] Refactor nightly test structure (#5479)
[3/N][Nightly] Move ops tests to nightly (#5538)
[Doc] Add new contributors. (#5537)
[2/N] Upgrade nightly doc (#5534)
[smoke][bugfix] moe_init_routing_v2 active_expert_range use int type (#5521)
[main][test] Refactor the mtp and eagle test case (#5326)
[Feature] Refactor PCP &DCP related code (#5214)
[Main2Main] Upgrade vllm commit to 1230 (#5495)
[Bugfix] Fix mm_merge (#5249)
[feature] mooncake support pcp/dcp in common conditions (#5224)
[Feature] Support kv nz feature for DeepSeek decode node in disagg-prefill scenario (#3072)
[Refactor] Formatting output types related to FuseMoE (#5481)
[P/D] Improve the performance of Layerwise Connector (#5303)
[Bugfix] fix the precision issues that may raise from the inter-layer reuse of the workspace in certain scenarios (#5522)
[Model] Add LongCat-Flash (#3833)
[Graph][Fusion] Add AddRMSNorm(with bias) (#5491)
[P/D] Bugfix zmq send/receive failed (#5503)
[Nightly] Trigger image build for nightly (#5547)
Bump actions/upload-artifact from 4 to 6 (#5466)
Bump actions/download-artifact from 4 to 7 (#5465)
[CI] Add multi-nodes longseq configs of DeepSeek-R1-W8A8 & Qwen3-235B-W8A8 (#5381)
Cleanup pass config override (#5283)
[Doc] Fix spelling mistake of environment variable name ASCEND_RT_VISIBLE_DEVICES in Doc (#5570)
[Feat][main] Supported to use full-graph with Qwen3-Next-MTP (#5477)
[Feat] enable hierarchical mc2 ops on A2 by default (#5545)
[CI] Move longseq Nightly CI (#5577)
[Perf][PCP][DCP] add multi-stream for GQA to enable computation-communication overlap (#5382)
[Recover] [Bugfix] support mtp kv transfer and pp partition by hand in kv transfer (#4892) (revert in #4981) (#4892)
[Doc] Fix typo in ASCEND_RT_VISIBLE_DEVICES (#5581)
[bugfix](pcp) expand max_num_tokens for pcp pad (#5478)
[BugFix]Disable dispatch_gmm_combine_decode operator when mtp drafter model uses non-w8a8 while main model uses w8a8, or drafter model is eagle series (#5293)
[KVPOOL]decode save kvcache (#5168)
[refactor](UT,PCP,DCP) refactor pcp&dcp patches in UTs (#5505)
[Doc]modify the quantization user guide and add a quantization adaptation developer guide (#5554)
[bugfix]update bishengir source envs (#5582)
[Bugfix] Fix chunk prefill bug for long_sequence feature (#5444)
[Bugfix] Fix weight transpose in RL scenarios (#5567)
[Doc] update supported models (#5379)
[CI] skip xlite-decode-only e2e test (#5407)
[Doc] eval-type not support service but server (#2920)
MLA prefill preformance optimization (#5456)
[Refactor][EAGLE] 2/N: load model and generate token (#5437)
[Bugfix] fix pcp + eplb error (#5561)
[Doc] add new doc for mooncake: PD-Colocated cross-node multi-instance validation of Mooncake's KV Cache reuse and performance. (#5415)
[BugFix][kernel] fix matmul_allreduce_add_rmsnorm_kernel (#5335)
feat: implement high-performance Triton kernels for rejection sampling: optimization for rejection_random_sample_kernel (#5259)
[Feat][Spec] Optimize token index calculation in spec decode with Triton kernel (#5356)
[Refactor]7/N Extract common code to common_cp (#5490)
[BugFix][Fusion] Fix graph fusion failure problem (#5253)
Add the requirement of arctic-inference which  speculative decoding with suffix_decode (#5045)
[Doc] Add NNAL installation guide and requirements (#5235)
Docs: Add A3 Docker image guidance for Atlas A3 machines (#5256)
[CI] Download models from ms (#5405)
[UT]add triton ops ut :  test_fused_qkvzba_split_reshape_cat (#5474)
[bugfix] fix test_camem failed with triton-ascend (#5492)
[Bugfix] record cos and sin cache in AscendRotaryEmbedding (#5516)
[P/D]Remove mooncake kvpool unused parameter `local_hostname` (#5574)
[CI] update triton-ascend version (#5584)
[docs] Correct image about prefill phase of PCP (#5598)
[perf] Fix MLAPO weight disposal for KV-consumer MLA in PD-mix deploy... (#5192)
[TRITON][TEST]Add nightly test for triton split_qkv_rmsnorm_rope (#5267)
Revert "[Feat] enable hierarchical mc2 ops on A2 by default (#5545)" (#5545)
[BugFix] Fix Smoke Testing Bug for DSR1 longseq (#5613)
[CI] mv ops to correct path (#5615)
[Main2Main] Upgrade vllm commit to 0105 (#5595)
[UT][PCP&DCP] UT for block_table.py (#5032)
[CI]update bisheng version (#5621)
[Main2Main] Upgrade vllm commit to 0106 (#5617)
[CI] Specify the version of xlite (#5612)
[MM][Bugfix] Update `hf_config` to `hf_text_config` (#5319)
[Refactor][EAGLE] 3/N delete redundant methods in mtp_proposer (#5420)
Bugfix: Align expert map shapes with redundant experts in EPLB adjustment (#5285)
[Bugfix] Remove swa parameter of fia (#5602)
[Nightly][Test] Add Qwen3-Next-80B-A3B-Instruct-W8A8 nightly test (#5616)
[Misc] Remove useless weight loader patch (#5619)
[P/D] Performance enhancement of Layerwise connector in TP asymmetric scenarios (#5540)
Revert "[BugFix][Fusion] Fix graph fusion failure problem (#5253)" (#5253)
[Bugfix] fix dcp_only bug and add e2e accuracy test for dcp only and pcp only (#5565)
[Graph][Fusion] Add AddRMSNormSPPattern and AddRMSNormSPPatternWithBias (#5569)
[Feature] implement basic framework for batch invariant (#5517)
[Refactor] Cleanup platform (#5566)
[bugfix (pcp)] fix chunked prefill accurancy issue (#5647)
[CI] Add DeepSeek-V3.2-W8A8 nightly ci test (#5371)
[Feature]EPLB:Adapt DispatchGmmCombineDecode operator to eplb tensor list and expert token numbers (#5552)
[Bugfix] Revert pr4214 multi-stream collect expert hotpot (#5529)
[Bugfix]Add register_kv_cache in ucm_connector (#5657)
[misc]Add Kimi-K2 series to CI model list (#5656)
[CI] cleanup single/multi-card test (#5623)
[CI] Bump lm-eval version to v0.4.9.2 (#5655)
[CI] Add workflow to cancel running workflows on PR close (#5646)
[Bugfix] fix resource are insufficient when pcp and piecewise (#5377)
[Bugfix] Fix the graph capture failure issue in the eagle3+full scenario. (#5553)
[CI] move image and wheel job to schedule way (#5685)
[Refactor] Fix AttentionMaskBuilder singleton and remove redundant pcp_prefill_mask (#4870)
[Refactor] Import global var form vllm instead of overwirte it (#5469)
[Tests] Add qwen3-8b nightly test (#5597)
[BugFix][Fusion] Fix graph fusion failure problem (#5676)
[1/N][CI] Refactor accuracy test (#5400)
[Kernel] Add moe_gating_top_k operator support for Ascend NPU (#5579)
[BugFix][P/D] Fix pre-create link parameter error (#5694)
[refactor] Refactor the interface for shard weight and remove the flashcomm2 o_shared interface. (#5181)
[bugfix] adapt to new implemented get_kv_cache_spec in cpuoffload connector (#4311)
[Feature] add the magicmtp speculative decoding acceleration algorithm (#5542)
Optimize the print info format when deprecated code is used in vllm-ascend (#5696)
[CI] fix image build tag (#5703)
[EPLB][CI] EPLB add aclgraph and redundant expert ci (#5625)
[CI] Drop outdated cases (#5709)
[CI] Fix image build workflow_dispatch error (#5717)
[Feat][Bugfix][main] Adapted SP to eagle3 (#5562)
[bugfix] Support dsv3.2 enable both mtp and full_decode_only (#5679)
[Doc] Add Qwen3-Omni-30B-A3B-Thinking Tutorials (#3991)
[Fix] Fixes speculative decode indexing and unpad condition for attention metadata (#5626)
[CI] Add triton ascend in nightly CI (#5716)
[feature]dcp&pcp support mlapo (#5672)
[CI] Remove workflow_dispatch way for image build (#5742)
[Nightly] Move ops to the correct path (#5642)
[OP] Enable custom op aclnnMoeInitRoutingCustom (#5332)
[CI] Add qwen3 next ci (#5395)
[Doc] add PaddleOCR-VL tutorials guide (#5556)
[BugFix][DS 3.2] Fix ds indexer accuracy problem caused by rope. (#4641)
[Doc][fix] Fix the title of the document for the layer_sharding feature (#5759)
[CI] lint and ut use self_hosted runner (#5652)
[BugFix] NetLoader: No backend type associated with device type npu (#5700)
[CI] Accuracy issue of qwen3-next-w8a8 nightly test fix. (#5746)
[BugFix] Xlite: Bypass the padding of the graph mode in non-MTP cases to obtain the correct decode num. (#5711)
[CustomOp] support TensorList for dispatchFFNCombine (#5665)
[CI] Avoid lint and ut for PR push (#5762)
[BufFix]Fix the error when using Ascend custom operators with rank=128 (#5394)
[Refactor] Replace the implementations of o_proj, q_b_proj, and kv_b_proj with custom_op for sharded CP (#5698)
[Bugfix] Fix matmul allreduce precision issue by using original weight (#4939)
[Feature] GLM4.6 support mtp with fullgraph (#5460)
[CI]Add Disaggregated PD Nightly Test for  Qwen3-235B and Qwen3-VL-235B (#5502)
support mxfp8 quantization (qwen dense) (#5723)
[Doc] Add GLM4.5 GLM4.6 doc (#5740)
[bugfix] Fixing KV Pool Memory Retention and Performance Degradation Issues (#5751)
[P/D][bugfix]Fix the PCP port mapping error issue (#5706)
[Feat] flashcomm2+oshard Generalized (#4723)
adapt to minimax_m2 (#5624)
[P/D] layerwise connector supports DeepSeek-V3.2 sparse attention && Distribute transfer tasks to redundant kv_head cards (#5722)
[main][bugfix] Fix fullgraph padding bug in mtp eagle refactor (#5692)
[Perf] Supports compute-communication overlap in the forward of sfa_v1 in the Sharded-CP feature. (#5701)
[Feature] Support for cross-attention and whisper model (#5592)
[0.13.0][doc] correct doc url (#5791)
[0.13.0][CI] disable main CI (#5792)
[0.13.0][Cherry Pick] cherry pick from 5638 Update pd readme (#5811)
[0.13.0][cherry-pick][bugfix](cp) align max_context_chunk to cp_virtual_block_size (#5782)
[0.13.0][Bugfix] bugfix for the order of dummy run pad and sync (#5778)
[0.13.0][Patch] AscendLoRAModelManager.__init__ (#5800)
[cherry-pick][BugFix] Support setting tp=1 for the Eagle draft model to take effect (#5804)
[v0.13.0][Bugfix] Support ALL D-Nodes in fullgraph when running MTP in PD (#5786)
[0.13.0][cherry-pick]enable ep32 for dispatch_ffn_combine (#5788)
[P/D] [CherryPick] 5846 fix layerwise connector for decoder tp size > num kv he… (#5857)
[0.13.0][cherry-pick][bugfix]Synchronize memcache adaptation on A2 (#5842)
[0.13.0][cherry-pick][Bugfix] Fixed an accuracy problem of sp with eagle3 (#5814)
[0.13.0][cherry-pick][P/D] bugfix for p node force free requset (#5431) (#5431)
[0.13.0][cherry-pick][bugfix](cp) replace None with zeros/inf tensor to avoid TypeError (#5844)
[v0.13.0][bugfix] patch set cudagraph size (#5860)
[v0.13.0][Bugfix] Fix acc bug when enbale dispatch_gmm_combine_decode and eplb[RFC: issue 5476] (#5836)
[0.13.0][Bugfix] Fix memory inconsistency in cross-process shared memory (#5779)
[v0.13.0][cherry-pick][BugFix] Fix DispatchGmmCombineDecode acc bug when big batch (#5873)
[v0.13.0][bugfix]Fix graph sync (#5809)
[0.13.0][cherry-pick][bugfix]support dsv3.2 enable both mtp and full_decode_only (#5849) (#5849)
Revert "[BugFix] Support setting tp=1 for the Eagle draft model to take effect(#5519) (#5519)
enbale qwen3-vl model fc1 feature (#5848)
Revert "[v0.13.0][bugfix]Fix graph sync (#5809)" (#5809)
[Performance]use triton mrope for Qwen3-VL (#5827)
[P/D]The issue of solving the force-free secondary release request, which causes the node to crash. (#5970)
[0.13.0][bugfix] fix mooncake kv cache transfer when one P has multi nodes (#5961)
[0.13.0][Feature] Support fine-grained shared expert overlap (#5962)
[0.13.0][Bugfix] fix bug of pcp+mtp+async scheduler (#5995)
[0.13.0][Bugfix] Add `synced_cudagraph_mode` to limit mixed graph modes in dp ranks (#6011)
【0.13.0】【bugfix】Resolved memory deallocation failure in the pooling layer under re-computation workloads. (#6056)
[0.13.0][cherry-pick][bugfix] fix bug of triton mrope (#6009)
[0.13.0][Bugfix] fix pcp aclgraph qwen FIA bug (#6038)
[Bugfix]Fixed precision issues caused by pooled request pooling (#6057)
[0.13.0][Bugfix] Fixed an problem related to embeddings sharing (#5972)
[v0.13.0][Bugfix] Fix XliteModelRunner init failed when aclgraph is enabled (#5887)
[0.13.0][Bugfix] Fix setting of `speculative_config.enforce_eager` for dsv32 (#5958)
Revert "[0.13.0][cherry-pick][bugfix] fix bug of triton mrope" (#6075)
[0.13.0][cherry-pick][bugfix] fix the complex and potentially problematic generate_kv_idx. (#5955)
[0.13.0][CI]fix for CI lint (#6093)
[EPLB][Bugfix] Dispatch Allgather use log2phy if enable eplb (#5933) (#5933)
[EPLB][Bugfix][v0.13.0] Incorporate the warm up of the EPLB into the profile run. (#6099)
[0.13.0][Doc] Supplement PD separation parameters of DeepSeek V3.1 (#6054)
[v0.13.0][CI] Upgrade to CANN 8.5.0 (#6101)
[0.13.0][Bugfix] Fix Triton operator usage for multimodal models based on `the mrope_interleaved` parameter (#6074)
[v0.13.0][BugFix][Cherry Pick] Fix input parameter bug of dispatch_gmm_combine_decode (#5931)
[Feature][Cherry Pick]Enable DispatchGmmCombineDecode when eagle is moe with w8a8, or not moe (#6081)
[v0.13.0][Bugfix] Fix the input constraints checks for the mlapo and bmm_transpose operators (#5764) (#5764)
[EPLB] Config Rename wrapper (#6111)
[v0.13.0][cherry-pick][BugFix]converting pa get_workspace back to capturing (#6108)
[cherry-pick][BugFix] Support setting tp=1 for the Eagle draft model to take effect (#6095)
[kv_cache] support multi_block_pool (#6106)
[CI] Skip some persistently stuck ut cases (#6133)
[0.13.0][CI]Add triton ascend version to 3.2.0 (#6105)
[0.13.0][cherry-pick][CP&SP] Integrate FIA operator in mla_cp._forward_decode (#6046)
[0.13.0][cherry-pick][BugFix] fix 3vl dense model load quant weight (#6103)
[0.13.0][cherry-pick] Reset incompatible config (#6118)
[0.13.0][Bugfix] Remove `use_aclgraph` in mtp_proposer and use `use_cuda_graph` (#6102)
[0.13.0][BugFix][cherry-pick]hccl bufferSize check for dispatch_ffn_combine (#6131)
Mix placement (#6086)
[v0.13.0][Feature] Support DSA-CP for Hybrid scenario (#5702) (#5702)
[0.13.0][P/D][PCP]bugfix pcp force free twice caused logger error (#6132)
[EPLB][Bugfix] Do not refresh parameters when eplb_config is not passed (#6160)
[0.13.0][Doc] update supported features (#6150)
[0.13.0][cherry-pick][CP&SP] Remove CP Redundant Variables after FIA operator enables for CANN 8.5 (#6039)
[v0.13.0][cherry-pick][Bugfix] Fix seq_lens reset issue causing performance degradation (#6166)
[0.13.0][Bugix] fix kv pcp+pooling+pd separation bug (#6152)
[Doc] Document translation (#6066)
[v0.13.0][bugfix] fix capture shape in sp_eagle_fullgraph (#6159)
[0.13.0][Feat] Merge the multi eagle graphs to one graph (#6178)
[0.13.0][BugFix] Avoided a bug of `torch_npu.npu_mm_reduce_scatter_base` when sp size >= 16 (#6167)
[0.13.0][BugFix]bug fix for dispatch_ffn_combine (#6157)
[0.13.0]Add has_connector_metadata (#6154)
[v0.13.0]skip eagle dp allreduce (#6162)
[0.13.0][KVCache] Support different page sizes (#6171)
[Bugfix] Fix the issue of the acceptance rate decline for Qwen3-30B-A3B-EAGLE3 (#6139)
[0.13.0][cherry-pick] addrmsnorm op support bias (#6140)
[Doc] Refresh doc for 0.13.0 (#6184)
[Doc] Add release note for 0.13.0rc2 (#6208)
[0.13.0] [BugFix] buildwheel dependency install (#6211)
[ci] Fix docker image build (#6215)
[0.13.0] [BugFix] Fix build wheel (#6220)
[Bugfix][v0.13.0] Fix a bug when cherry-pick from main (#6209)
[0.13.0][Bugfix] Avoided a bug of drafter when `dp` and `sp` are enabled (#6224)
[Inductor][v0.13.0]Adapt AddRmsNormQuant pass to new addrmsnormBias operator (#6210)
[v0.13.0][cherry-pick][BugFix] Fix moe_load accumulation error in ACL graph mode (#6258)
[CI] migrate single card runner to hk (#6260)
[Refactor] use the count of kv_cache_group to create multi_block_table (#6116)
[Misc][v0.13.0]Removes unnecessary graph size re-initialization (#6281)
[0.13.0][KVCache] Prioritize using a hybrid manager to manage different types of kvcache (#6289)
[CI] Update pta to 2.8.0.post2 (#6287)
[0.13.0][cherry-pick][BugFix][CI]Fix DeepSeek-R1-W8A8-longseq nightly CI (#6337)
[CI] Add per pr image build for nightly test (#6353)
[CI] Cherry pick nightly test from main (#6365)
[cherry-pick][BugFix] Disable enable_shared_expert_dp by default if tensor_parallel_size=1 (#6363)
[0.13.0][Bugfix] Fix hash conflict due to reset incompatible configuations (#6330)
[0.13.0][Bugfix] Fix FIA operator validation error in Eagle scenario with CANN 8.5 (#6284)
[CI]Limit transformers version (#6373)
[Doc] Reranker guide remove deprecate task option (#6380)
[0.13.0][cherry-pick][bugfix](CP,MLA) fix wrong slot_mapping of decode for mixed p/d batch (#6346)
[0.13.0][Profiler] Fix profiler bug (#6383)
[0.13.0][cherry-pick][bugfix](pcp,gqa) set kv_inverse_idx_for_chunk and cp_kv_recover_idx_for_chunk to None when dcp only (#6318)
[0.13.0][Bugfix]Fix of Pooling Code (#6146)
[0.13.0][cherry-pick][Bugfix][CI] Specify tensorflow version in accuracy test to avoid segmentation fault (#6292) (#6292)
[0.13.0][bugfix]Raise exception for Omni models with FLASHCOMM enabled (#6392)
[CI][Nightly] Correct the nightly image build ref (#6396)
[P/D][0.13.0]Add ssl cert for metaserver proxy (#6400)
[0.13.0]fix patch cudagraph size (#6397)
fix: resolve sync bug in DispathFFNCombine when expert num per card is 32 (#6422)
work around: reset the None reference count to prevent it from droppi… (#6441)
[v0.13.0][Eagle3]Extend PR #5786 to eagle3 (#6443)
[0.13.0][cherry-pick]pick from 6310 to fix rope op (#6444)
[CI] change ds32 cudagraph_sizes (#6399)
[bugfix][0.13.0]fix bug in dispatch_ffn_combine kernel (#6464)
[v0.13.0][Lora][BugFix] Fix crash on base model requests with LoRA enabled (#6457)
Bugfix: Pre-compile EPLB algorithm successfully in subprocess under graph mode (#6472)
[0.13.0][Bugfix] fix npu memory is not released in cp (#6479)
[0.13.0][Bugfix] Fix problematic dummy_run & improper input_batch_size in eagle (#6518)
